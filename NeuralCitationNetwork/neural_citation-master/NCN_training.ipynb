{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ncn.model import *\n",
    "from ncn.training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:ncn.data:Getting fields...\n",
      "INFO:ncn.data:Loading dataset...\n",
      "INFO:ncn.data:Building vocab...\n"
     ]
    }
   ],
   "source": [
    "# set up training\n",
    "#data = get_bucketized_iterators(\"/home/jupyter/tutorials/seminar_kd/arxiv_data.csv\",\n",
    "data = get_bucketized_iterators(\"ncn/arxiv_data.csv\",\n",
    "                                batch_size = 64,\n",
    "                                len_context_vocab = 20000,\n",
    "                                len_title_vocab = 20000,\n",
    "                                len_aut_vocab = 20000)\n",
    "PAD_IDX = data.ttl.vocab.stoi['<pad>']\n",
    "cntxt_vocab_len = len(data.cntxt.vocab)\n",
    "aut_vocab_len = len(data.aut.vocab)\n",
    "ttl_vocab_len = len(data.ttl.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\thi.dn\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NeuralCitationNetwork(\n",
       "  (encoder): NCNEncoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (context_embedding): Embedding(20002, 128, padding_idx=1)\n",
       "    (context_encoder): TDNNEncoder(\n",
       "      (encoder): ModuleList(\n",
       "        (0): TDNN(\n",
       "          (conv): Conv2d(1, 256, kernel_size=(128, 4), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): TDNN(\n",
       "          (conv): Conv2d(1, 256, kernel_size=(128, 4), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): TDNN(\n",
       "          (conv): Conv2d(1, 256, kernel_size=(128, 5), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): TDNN(\n",
       "          (conv): Conv2d(1, 256, kernel_size=(128, 6), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (4): TDNN(\n",
       "          (conv): Conv2d(1, 256, kernel_size=(128, 7), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (author_embedding): Embedding(20002, 128, padding_idx=1)\n",
       "    (citing_author_encoder): TDNNEncoder(\n",
       "      (encoder): ModuleList(\n",
       "        (0): TDNN(\n",
       "          (conv): Conv2d(1, 256, kernel_size=(128, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): TDNN(\n",
       "          (conv): Conv2d(1, 256, kernel_size=(128, 2), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (cited_author_encoder): TDNNEncoder(\n",
       "      (encoder): ModuleList(\n",
       "        (0): TDNN(\n",
       "          (conv): Conv2d(1, 256, kernel_size=(128, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): TDNN(\n",
       "          (conv): Conv2d(1, 256, kernel_size=(128, 2), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (attention): Attention(\n",
       "    (attn): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(20004, 128, padding_idx=1)\n",
       "    (rnn): GRU(384, 256, dropout=0.2)\n",
       "    (out): Linear(in_features=640, out_features=20004, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "net = NeuralCitationNetwork(context_filters=[4,4,5,6,7],\n",
    "                            author_filters=[1,2],\n",
    "                            context_vocab_size=cntxt_vocab_len,\n",
    "                            title_vocab_size=ttl_vocab_len,\n",
    "                            author_vocab_size=aut_vocab_len,\n",
    "                            pad_idx=PAD_IDX,\n",
    "                            num_filters=256,\n",
    "                            authors=True, \n",
    "                            embed_size=128,\n",
    "                            num_layers=1,\n",
    "                            hidden_size=256,\n",
    "                            dropout_p=0.2,\n",
    "                            show_attention=False)\n",
    "net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:ncn.training:INITIALIZING NEURAL CITATION NETWORK WITH AUTHORS = True\nRunning on: cpu\nNumber of model parameters: 24,341,796\nEncoders: # Filters = 256, Context filter length = [4, 4, 5, 6, 7],  Context filter length = [1, 2]\nEmbeddings: Dimension = 128, Pad index = 1, Context vocab = 20002, Author vocab = 20002, Title vocab = 20004\nDecoder: # GRU cells = 1, Hidden size = 256\nParameters: Dropout = 0.2, Show attention = False\n-------------------------------------------------\nTRAINING SETTINGS\nSeed = 34, # Epochs = 20, Batch size = 64, Initial lr = 0.001\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d75770af09c40659143e97d65cbab5f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training batches:   0%|          | 0/6280 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95ca56feefe94417a442a56e6aa64813"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating batches:   0%|          | 0/785 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d86ef495953446c790647ffa7bdba0fd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:ncn.training:Epoch: 01 | Time: 232m 40s\n",
      "INFO:ncn.training:\tTrain Loss: 5.028\n",
      "INFO:ncn.training:\t Val. Loss: 4.284\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training batches:   0%|          | 0/6280 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf11d515f90646bfb480d082155f6445"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating batches:   0%|          | 0/785 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d92dd3c0e540425f8f98ddac093bbd98"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:ncn.training:Epoch: 02 | Time: 218m 58s\n",
      "INFO:ncn.training:\tTrain Loss: 4.196\n",
      "INFO:ncn.training:\t Val. Loss: 3.971\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training batches:   0%|          | 0/6280 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1eb112bf7b064a979cf4bf56936880fb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating batches:   0%|          | 0/785 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aca09116b7aa41c1bc3a9473d2b49f2a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:ncn.training:Epoch: 03 | Time: 237m 36s\n",
      "INFO:ncn.training:\tTrain Loss: 3.949\n",
      "INFO:ncn.training:\t Val. Loss: 3.824\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training batches:   0%|          | 0/6280 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e740b8f55644544a73194db3aed1c25"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4d1d542773b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_losses, valid_losses = train_model(model = net, \n\u001b[0m\u001b[0;32m      2\u001b[0m                                          \u001b[0mtrain_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                          \u001b[0mvalid_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                          \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                          \u001b[0mpad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPAD_IDX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Thi.DN\\PhD\\ThayBay\\GCN\\ebookML_src-master\\src\\neural_citation-master\\ncn\\training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_iterator, valid_iterator, pad, model_name, n_epochs, clip, lr, save_dir)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mtraining_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Thi.DN\\PhD\\ThayBay\\GCN\\ebookML_src-master\\src\\neural_citation-master\\ncn\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mttl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = train_model(model = net, \n",
    "                                         train_iterator = data.train_iter, \n",
    "                                         valid_iterator = data.valid_iter,\n",
    "                                         lr = 0.001,\n",
    "                                         pad = PAD_IDX,\n",
    "                                         model_name = \"embed_128_hid_256_1_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}