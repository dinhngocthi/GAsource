{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from typing import List, Set\n",
    "from functools import partial\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from torchtext.data import Field, BucketIterator, Dataset, TabularDataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Base logger for the neural citation package.'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, style='$')\n",
    "logger = logging.getLogger(\"neural_citation\")\n",
    "\"\"\"Base logger for the neural citation package.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "tokenizer = Tokenizer(nlp.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords() -> Set:\n",
    "    \"\"\"\n",
    "    Returns spacy and nltk stopwords unified into a single set.   \n",
    "    \n",
    "    ## Output:  \n",
    "    \n",
    "    - **STOPWORDS** *(Set)*: Set containing the stopwords for preprocessing \n",
    "    \"\"\"\n",
    "    STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    nltk_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    STOPWORDS.update(nltk_stopwords)\n",
    "    return STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_context_preprocessing(text: str, tokenizer: Tokenizer, identifier:str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Applies the following preprocessing steps on a string:  \n",
    " \n",
    "    1. Replace digits\n",
    "    2. Remove all punctuation.  \n",
    "    3. Tokenize.  \n",
    "    4. Remove numbers.  \n",
    "    5. Lemmatize.   \n",
    "    6. Remove blanks  \n",
    "    7. Prune length to max length (different for contexts and titles)  \n",
    "    \n",
    "    ## Parameters:  \n",
    "    \n",
    "    - **text** *(str)*: Text input to be processed.  \n",
    "    - **tokenizer** *(spacy.tokenizer.Tokenizer)*: SpaCy tokenizer object used to split the string into tokens.      \n",
    "    - **identifier** *(str)*: A string determining whether a title or a context is passed as text.  \n",
    "\n",
    "    \n",
    "    ## Output:  \n",
    "    \n",
    "    - **List of strings**:  List containing the preprocessed tokens.\n",
    "    \"\"\"\n",
    "    text = re.sub(\"\\d*?\", '', text)\n",
    "    text = re.sub(\"[\" + re.escape(string.punctuation) + \"]\", \" \", text)\n",
    "    text = [token.lemma_ for token in tokenizer(text) if not token.like_num]\n",
    "    text = [token for token in text if token.strip()]\n",
    "\n",
    "    # return the sequence up to max length or totally if shorter\n",
    "    # max length depends on the type of processed text\n",
    "    if identifier == \"context\":\n",
    "        try:\n",
    "            return text[:100]\n",
    "        except IndexError:\n",
    "            return text\n",
    "    elif identifier == \"title_cited\":\n",
    "        try:\n",
    "            return text[:30]\n",
    "        except IndexError:\n",
    "            return text\n",
    "    else:\n",
    "        raise NameError(\"Identifier name could not be found.\")\n",
    "\n",
    "\n",
    "def author_preprocessing(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Applies the following preprocessing steps on a string:  \n",
    "\n",
    "    \n",
    "    1. Remove all numbers.   \n",
    "    2. Tokenize.  \n",
    "    3. Remove blanks.  \n",
    "    4. Prune length to max length. \n",
    "    \n",
    "    ## Parameters:  \n",
    "    \n",
    "    - **text** *(str)*: Text input to be processed.  \n",
    "    \n",
    "    ## Output:  \n",
    "    \n",
    "    - **List of strings**:  List containing the preprocessed author tokens. \n",
    "    \"\"\"\n",
    "    text = re.sub(\"\\d*?\", '', text)\n",
    "    text = text.split(',')\n",
    "    text = [token.strip() for token in text if token.strip()]\n",
    "\n",
    "    # return the sequence up to max length or totally if shorter\n",
    "    try:\n",
    "        return text[:5]\n",
    "    except IndexError:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = get_stopwords()\n",
    "cntxt_tokenizer = partial(title_context_preprocessing, tokenizer=tokenizer, identifier=\"context\")\n",
    "ttl_tokenizer = partial(title_context_preprocessing, tokenizer=tokenizer, identifier=\"title_cited\")\n",
    "\n",
    "# instantiate fields preprocessing the relevant data\n",
    "TTL = Field(tokenize=ttl_tokenizer, \n",
    "            stop_words=STOPWORDS,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>',\n",
    "            lower=True)\n",
    "\n",
    "AUT = Field(tokenize=author_preprocessing, batch_first=True, lower=True)\n",
    "\n",
    "CNTXT = Field(tokenize=cntxt_tokenizer, stop_words=STOPWORDS, lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:neural_citation:Getting fields...\n",
      "INFO:neural_citation:Loading dataset...\n",
      "INFO:neural_citation:Building vocab...\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Getting fields...\")\n",
    "# generate torchtext dataset from a .csv given the fields for each datatype\n",
    "# has to be single dataset in order to build proper vocabularies\n",
    "logger.info(\"Loading dataset...\")\n",
    "#dataset = TabularDataset(\"arxiv_data.csv\", \"CSV\", \n",
    "dataset = TabularDataset(\"D:/Thi.DN/PhD/ThayBay/GCN/ebookML_src-master/src/neural_citation-master/ncn/arxiv_data.csv\", \"CSV\", \n",
    "                   [(\"context\", CNTXT), (\"authors_citing\", AUT), (\"title_cited\", TTL), (\"authors_cited\", AUT)],\n",
    "                   skip_header=True)\n",
    "\n",
    "# build field vocab before splitting data\n",
    "logger.info(\"Building vocab...\")\n",
    "TTL.build_vocab(dataset, max_size=20000)\n",
    "AUT.build_vocab(dataset, max_size=20000)\n",
    "CNTXT.build_vocab(dataset, max_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = dataset.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "502353"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aut_matchings(examples):\n",
    "    mapping = {}\n",
    "    for example in examples:\n",
    "        key = tuple(example.title_cited)\n",
    "        if key not in mapping.keys():\n",
    "            mapping[key] = example.authors_cited\n",
    "    \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_aut = get_aut_matchings(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"title_to_aut_cited.pkl\", \"wb\") as fp:\n",
    "with open(\"title_to_aut_cited_Thi.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(mapping_aut, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat = pd.read_csv(\"arxiv_data.csv\")\n",
    "dat = pd.read_csv(\"D:/Thi.DN/PhD/ThayBay/GCN/ebookML_src-master/src/neural_citation-master/ncn/arxiv_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[\"ttl_proc\"] = dat[\"title_cited\"].map(lambda x: TTL.preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            ttl_proc  \\\n",
       "0            [detecting, read, text, natural, scene]   \n",
       "1                 [icdar, competition, robust, read]   \n",
       "2                 [icdar, competition, robust, read]   \n",
       "3         [icdar, text, locate, competition, result]   \n",
       "4                 [icdar, competition, robust, read]   \n",
       "5            [detecting, read, text, natural, scene]   \n",
       "6  [fully, convolutional, network, semantic, segm...   \n",
       "7                        [long, short, term, memory]   \n",
       "8  [connectionist, temporal, classification, labe...   \n",
       "9  [photoocr, reading, text, uncontrolled, condit...   \n",
       "\n",
       "                                         title_cited  \n",
       "0       Detecting and reading text in natural scenes  \n",
       "1            ICDAR2015 competition on robust reading  \n",
       "2            ICDAR2015 competition on robust reading  \n",
       "3       Icdar 2005 text locating competition results  \n",
       "4            ICDAR2015 competition on robust reading  \n",
       "5       Detecting and reading text in natural scenes  \n",
       "6  Fully convolutional networks for   semantic se...  \n",
       "7                             Long short-term memory  \n",
       "8  Connectionist temporal   classification: Label...  \n",
       "9  PhotoOCR: Reading text in   uncontrolled condi...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ttl_proc</th>\n      <th>title_cited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[detecting, read, text, natural, scene]</td>\n      <td>Detecting and reading text in natural scenes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[icdar, competition, robust, read]</td>\n      <td>ICDAR2015 competition on robust reading</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[icdar, competition, robust, read]</td>\n      <td>ICDAR2015 competition on robust reading</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[icdar, text, locate, competition, result]</td>\n      <td>Icdar 2005 text locating competition results</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[icdar, competition, robust, read]</td>\n      <td>ICDAR2015 competition on robust reading</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[detecting, read, text, natural, scene]</td>\n      <td>Detecting and reading text in natural scenes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[fully, convolutional, network, semantic, segm...</td>\n      <td>Fully convolutional networks for   semantic se...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[long, short, term, memory]</td>\n      <td>Long short-term memory</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[connectionist, temporal, classification, labe...</td>\n      <td>Connectionist temporal   classification: Label...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[photoocr, reading, text, uncontrolled, condit...</td>\n      <td>PhotoOCR: Reading text in   uncontrolled condi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "dat[[\"ttl_proc\", \"title_cited\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_to_full(data):\n",
    "    mapping = {}\n",
    "    for index in data.index:\n",
    "        key = \" \".join(data.iloc[index, 4])\n",
    "        if key not in mapping.keys():\n",
    "            mapping[key] = data.iloc[index, 2]\n",
    "    \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_titles = title_to_full(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"title_tokenized_to_full.pkl\", \"wb\") as fp:\n",
    "with open(\"title_tokenized_to_full_Thi.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(mapping_titles, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}